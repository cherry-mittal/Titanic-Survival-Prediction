
# ğŸš¢ Titanic Survival Prediction using Machine Learning

## Project Overview

The project builds a **machine learning model to predict passenger survival on the Titanic** using demographic and travel-related features. It is based on the classic Kaggle Titanic dataset and demonstrates an **endâ€‘toâ€‘end ML workflow**, including data preprocessing, exploratory data analysis (EDA), feature engineering, model training, evaluation, and prediction.

---

## Problem Statement

Given passenger information such as age, gender, ticket class, fare, and family size, predict whether a passenger **survived (1)** or **did not survive (0)** the Titanic disaster.

This is a **binary classification problem**.

---

## Dataset

* **Source:** Kaggle â€“ Titanic: Machine Learning from Disaster
* **Training set:** 891 rows Ã— 12 columns
* **Test set:** 418 rows Ã— 11 columns

### Key Features

| Feature  | Description                       |
| -------- | --------------------------------- |
| Pclass   | Passenger class (1st, 2nd, 3rd)   |
| Sex      | Gender of passenger               |
| Age      | Age in years                      |
| SibSp    | Number of siblings/spouses aboard |
| Parch    | Number of parents/children aboard |
| Fare     | Ticket fare                       |
| Embarked | Port of embarkation (C, Q, S)     |

Target Variable:

* **Survived** â†’ `1 = Survived`, `0 = Did not survive`

---

## Exploratory Data Analysis (EDA)

EDA was performed using:

* Distribution plots
* Survival rate comparisons
* Missing value analysis

---

## Data Preprocessing

The following preprocessing steps were applied:

* Missing value imputation (Age, Embarked)
* Dropping irrelevant features (e.g., PassengerId, Name, Ticket)
* Encoding categorical variables (Sex, Embarked)
* Feature scaling (where required)
* Creating derived features such as **FamilySize**

---

## Models Used

Multiple machine learning models were trained and evaluated:

* Logistic Regression
* Decision Tree Classifier
* Random Forest Classifier

The final model was selected based on **accuracy, precision, recall, and crossâ€‘validation performance**.

---

## Model Evaluation

Evaluation metrics used:

* Accuracy
* Confusion Matrix
* Precision, Recall, F1â€‘Score
* Crossâ€‘Validation Score

Example Results:

| Model               | Accuracy |
| ------------------- | -------- |
| Logistic Regression | ~78%     |
| Random Forest       | ~82â€“85%  |

*(Exact results may vary depending on hyperparameters and random seed.)*

---

## ğŸ“Œ Key Learnings

* Importance of feature engineering in classical ML
* Handling missing and categorical data effectively
* Comparing multiple models instead of relying on one
* Evaluating models beyond accuracy

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ™Œ Acknowledgements

* Kaggle Titanic Dataset
* Scikitâ€‘learn documentation
* Openâ€‘source ML community

---

## ğŸ‘¤ Author

**Cherry Mittal**
Machine Learning Enthusiast | AI & Data Science

If you find this project helpful, feel free to â­ the repository!


